{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 256\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),  # Dodajemy augmentację\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=Path('./afhq/train'), transform=transform)\n",
    "val_dataset = ImageFolder(root=Path('./afhq/val'), transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=64, image_channels=3, latent_dim=128):\n",
    "        \"\"\"\n",
    "        Inicjalizacja Wariacyjnego Autonenkodera (VAE)\n",
    "        Args:\n",
    "            image_size: rozmiar obrazu wejściowego (zakładamy kwadratowy obraz)\n",
    "            image_channels: liczba kanałów w obrazie (3 dla RGB, 1 dla skali szarości)\n",
    "            latent_dim: wymiar przestrzeni ukrytej\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Parametry sieci\n",
    "        self.image_size = image_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.image_channels = image_channels\n",
    "        \n",
    "        # Architektura enkodera - kolejne warstwy zwiększają liczbę kanałów i zmniejszają wymiary przestrzenne\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Blok 1: (image_channels, 64, 64) -> (64, 32, 32)\n",
    "            nn.Conv2d(image_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Blok 2: (64, 32, 32) -> (128, 16, 16)\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Blok 3: (128, 16, 16) -> (256, 8, 8)\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Blok 4: (256, 8, 8) -> (512, 4, 4)\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Blok 5: (512, 4, 4) -> (512, 2, 2)\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Obliczamy wymiar spłaszczonego tensora\n",
    "        self.flatten_size = 512 * 2 * 2\n",
    "        \n",
    "        # Warstwy dla rozkładu wariacyjnego\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.fc_var = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Warstwa wejściowa dekodera\n",
    "        self.decoder_input = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, self.flatten_size)\n",
    "        )\n",
    "        \n",
    "        # Dekoder - kolejne warstwy zmniejszają liczbę kanałów i zwiększają wymiary przestrzenne\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Blok 1: (512, 2, 2) -> (512, 4, 4)\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Blok 2: (512, 4, 4) -> (256, 8, 8)\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Blok 3: (256, 8, 8) -> (128, 16, 16)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Blok 4: (128, 16, 16) -> (64, 32, 32)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Blok 5: (64, 32, 32) -> (image_channels, 64, 64)\n",
    "            nn.ConvTranspose2d(64, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Normalizacja wyjścia do zakresu [0, 1]\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Enkodowanie obrazu do parametrów rozkładu w przestrzeni ukrytej\n",
    "        Args:\n",
    "            x: tensor obrazu wejściowego [batch_size, image_channels, image_size, image_size]\n",
    "        Returns:\n",
    "            mu: średnia rozkładu\n",
    "            log_var: logarytm wariancji rozkładu\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Reparametryzacja - pobieranie próbki z rozkładu\n",
    "        Args:\n",
    "            mu: średnia rozkładu\n",
    "            log_var: logarytm wariancji rozkładu\n",
    "        Returns:\n",
    "            próbka z rozkładu\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Dekodowanie z przestrzeni ukrytej do obrazu\n",
    "        Args:\n",
    "            z: wektor z przestrzeni ukrytej [batch_size, latent_dim]\n",
    "        Returns:\n",
    "            zrekonstruowany obraz\n",
    "        \"\"\"\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 512, 2, 2)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Przejście w przód przez sieć\n",
    "        Args:\n",
    "            x: tensor obrazu wejściowego [batch_size, image_channels, image_size, image_size]\n",
    "        Returns:\n",
    "            rekonstrukcja: zrekonstruowany obraz\n",
    "            mu: średnia rozkładu\n",
    "            log_var: logarytm wariancji rozkładu\n",
    "        \"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(DEVICE)\n",
    "model.load_state_dict(torch.load('vae_epoch_50.pth', map_location=DEVICE))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(image_size=IMAGE_SIZE, latent_dim=LATENT_DIM).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, log_var, kld_weight=0.0005):\n",
    "    # Strata rekonstrukcji - kombinacja MSE i L1\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    L1 = F.l1_loss(recon_x, x, reduction='sum')\n",
    "    reconstruction_loss = MSE + 0.5 * L1\n",
    "    \n",
    "    # Strata KL Divergence\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    \n",
    "    return reconstruction_loss + kld_weight * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, results_path):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_dataloader):\n",
    "        data = data.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item() / len(data):.4f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_dataloader.dataset)\n",
    "    print(f'Epoch: {epoch}, Average Loss: {avg_loss:.4f}')\n",
    "    print('--------------------------------------------')\n",
    "    torch.save(model.state_dict(), f'{results_path}/vae_epoch_{epoch}.pth')\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(epoch, n_samples=10, results_path=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(train_dataloader))\n",
    "        data = data[:n_samples].to(DEVICE)\n",
    "        recon, _, _ = model(data)\n",
    "        z = torch.randn(n_samples, LATENT_DIM).to(DEVICE)\n",
    "        generated = model.decode(z)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, i + 1)\n",
    "            plt.imshow(data[i].cpu().permute(1, 2, 0) * 0.5 + 0.5)\n",
    "            plt.axis('off')\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, i + 1 + n_samples)\n",
    "            plt.imshow(recon[i].cpu().permute(1, 2, 0) * 0.5 + 0.5)\n",
    "            plt.axis('off')\n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(3, n_samples, i + 1 + 2 * n_samples)\n",
    "            plt.imshow(generated[i].cpu().permute(1, 2, 0) * 0.5 + 0.5)\n",
    "            plt.axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_path}/results_epoch_{epoch}.png')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "training_indentifier = 'cats_reworked_vae_256'\n",
    "# create directory for training_indentifier\n",
    "\n",
    "results_path = Path(training_indentifier)\n",
    "results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train(epoch, results_path)\n",
    "    visualize_results(epoch, 10, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_images_advanced(model, image1_path, image2_path, steps=10, method='spherical'):\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    img1 = Image.open(image1_path).convert('RGB')\n",
    "    img2 = Image.open(image2_path).convert('RGB')\n",
    "\n",
    "    img1_tensor = transform(img1).unsqueeze(0).to(DEVICE)\n",
    "    img2_tensor = transform(img2).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu1, _ = model.encode(img1_tensor)\n",
    "        mu2, _ = model.encode(img2_tensor)\n",
    "\n",
    "        def linear_interpolation(v1, v2, alpha):\n",
    "            return v1 * (1 - alpha) + v2 * alpha\n",
    "\n",
    "        def spherical_interpolation(v1, v2, alpha):\n",
    "            v1_norm = F.normalize(v1, dim=1)\n",
    "            v2_norm = F.normalize(v2, dim=1)\n",
    "\n",
    "            dot = (v1_norm * v2_norm).sum(1, keepdim=True).clamp(-1.0, 1.0)\n",
    "            omega = torch.acos(dot)\n",
    "            sin_omega = torch.sin(omega)\n",
    "\n",
    "            if torch.any(sin_omega == 0):\n",
    "                return linear_interpolation(v1, v2, alpha)\n",
    "\n",
    "            term1 = torch.sin((1.0 - alpha) * omega) / sin_omega\n",
    "            term2 = torch.sin(alpha * omega) / sin_omega\n",
    "            return term1 * v1 + term2 * v2\n",
    "\n",
    "        interpolation_func = spherical_interpolation if method == 'spherical' else linear_interpolation\n",
    "\n",
    "        vectors = []\n",
    "        for alpha_val in np.linspace(0, 1, steps):\n",
    "            alpha_tensor = torch.tensor([[alpha_val]], dtype=mu1.dtype, device=DEVICE)\n",
    "            interpolated = interpolation_func(mu1, mu2, alpha_tensor)\n",
    "            vectors.append(interpolated)\n",
    "\n",
    "        fig, axes = plt.subplots(2, steps + 2, figsize=(20, 6))\n",
    "\n",
    "        recon1, _, _ = model(img1_tensor)\n",
    "        recon2, _, _ = model(img2_tensor)\n",
    "\n",
    "        axes[0, 0].imshow(img1)\n",
    "        axes[0, 0].axis('off')\n",
    "        axes[0, 0].set_title('Original 1')\n",
    "\n",
    "        recon_img1 = recon1[0].cpu().permute(1, 2, 0) * 0.5 + 0.5\n",
    "        axes[1, 0].imshow(recon_img1.clamp(0, 1))\n",
    "        axes[1, 0].axis('off')\n",
    "        axes[1, 0].set_title('Reconstructed 1')\n",
    "\n",
    "        for i, vec in enumerate(vectors):\n",
    "            decoded = model.decode(vec.to(DEVICE))\n",
    "            img = decoded[0].cpu().permute(1, 2, 0) * 0.5 + 0.5\n",
    "            axes[0, i + 1].axis('off')\n",
    "            axes[1, i + 1].imshow(img.clamp(0, 1))\n",
    "            axes[1, i + 1].axis('off')\n",
    "            if i == 0 or i == steps - 1:\n",
    "                axes[1, i + 1].set_title(f'Step {i + 1}')\n",
    "\n",
    "        axes[0, -1].imshow(img2)\n",
    "        axes[0, -1].axis('off')\n",
    "        axes[0, -1].set_title('Original 2')\n",
    "\n",
    "        recon_img2 = recon2[0].cpu().permute(1, 2, 0) * 0.5 + 0.5\n",
    "        axes[1, -1].imshow(recon_img2.clamp(0, 1))\n",
    "        axes[1, -1].axis('off')\n",
    "        axes[1, -1].set_title('Reconstructed 2')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'interpolation_{method}.png')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Przykład użycia funkcji interpolacji:\n",
    "interpolate_images_advanced(model, 'afhq/train/cat/flickr_cat_000003.jpg', 'afhq/train/cat/pixabay_cat_004569.jpg', \n",
    "                            steps=10, method='spherical')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
